{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ee1ac-e05f-42f8-b724-6aa2cb415edf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b9364-73c4-46bf-85dc-c31cfd38c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_file_from_path(path):\n",
    "    with open(os.path.expanduser(path), 'r', encoding='utf-8') as file:\n",
    "        return file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed622792-4344-4dc1-9158-716715a98304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"]=read_file_from_path('~/gemini_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ec9fb-ffea-4190-92bd-7dfce7896990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2cd535-e18e-43bb-bca8-cc8cfe6df96d",
   "metadata": {},
   "source": [
    "# Critique Chain\n",
    "\n",
    "User enters a question, LLM gets a response, and another LLM critiques the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2dc5146-070c-488f-ab5a-b82b777c151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = ChatPromptTemplate.from_template(\"{question}\")\n",
    "base_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "critic_prompt = ChatPromptTemplate.from_template(\"A user asked <Question>{question}</Question> and received the response <Response>{response}</Response> from an LLM. Critique the quality of this response.\")\n",
    "critic_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "critique_chain = (\n",
    "    RunnableParallel(question=RunnablePassthrough(), response=(base_prompt | base_model | StrOutputParser() | RunnableLambda(lambda s: s.strip())))\n",
    "    | RunnablePassthrough.assign(critic_prompt=critic_prompt)\n",
    "    | RunnablePassthrough.assign(critique=(RunnableLambda(itemgetter('critic_prompt')) | critic_model | StrOutputParser()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3bab9-b557-48f1-931e-2fd122b3c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(d):\n",
    "    divider = f\"\\n\\n---------------------------------------------------------\\n\\n\"\n",
    "    format = lambda header, text: f\"{header}\\n\\n{text}\"\n",
    "    return divider.join([format('USER', d['question']), format('LLM', d['response']), format('CRITIC', d['critique'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d9472-4440-425b-826b-c97ad7370ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "critique_chain_io = critique_chain | RunnableLambda(output)\n",
    "question = input()\n",
    "result = critique_chain_io.invoke(question)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
